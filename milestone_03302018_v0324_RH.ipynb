{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Optimizing Machine Learning Techniques for Automatic Nuclei Detection\n",
    "\n",
    "<b>1. Motivation</b>\n",
    "\n",
    "Many people die from cancer every year. This is terribly sad. :’( We would like to prevent this. One option is to identify the cancer early; treatment of nascent cancers tends to be more successful. Often, cells suffering from cancer display different biomarkers in their nuclei. These biomarkers can be seen in immunohistochemical (IHC) imaging. If there were a rapid and accurate way to screen images for cancerous cells, many cancer prognoses could be improved. We aim to develop an automated cell nuclei detection technique.\n",
    "\n",
    "<b>2. Challenges</b>\n",
    "\n",
    "Imaging conditions can vary tremendously - illumination, contrast, fluorescence and staining will all affect the appearance of the cell. We would also like our technique to be generalizable to many cell types because cancer is insidious and affects many cell types. Depending on the biological sample that was imaged, cells may also be aggregated; where a trained eye could distinguish individual nuclei, an algorithm might falter. \n",
    "\n",
    "<b>3. Pre-treatment and Initial Segmentation (“Detection”)</b>\n",
    "\n",
    "Colour is usually either normalized or thresholded to remove noise and background [1]. A variety of pre-processing techniques can be used to find objects, from conventional blob detection, to morphological and/or contour resolution, to watershed segmentation [2-4]. We will use sensible discretion to pick a pre-treatment technique.\n",
    "\n",
    "<b>4. Segmentation and Identification</b>\n",
    "\n",
    "There are two primary approaches to cell segmentation:\n",
    "\n",
    "<b><i>Traditional methods segment nuclei from single or overlapping cells</i></b>\n",
    "\n",
    "Several traditional methods, often involving a-priori knowledge of cell shape and size [5]. First, cell clusters are segmented from the background by concavity [6-7]. Next, cell clusters can be separated into individual cells based on the concavity of the intensity distribution [7]. After individual cells have been identified, cell boundaries are often approximated using elliptical curve-fitting techniques [5,7]. Further segmentation can be applied to separate the cell nucleus from the cytoplasm. A gradient vector flow active contour model (GVF-ACM) has been shown to find boundaries between the nucleus and cytoplasm [8].\n",
    "\n",
    "In this work, we can begin performing segmentation using the scikit-image package for python. This package includes methods for ellipse and boundary fitting, as well as edge detection and active contour modeling.\n",
    "\n",
    "<b><i>Machine learning methods identify nuclei via classification algorithms</i></b>\n",
    "\n",
    "Machine learning and pattern recognition have been successfully used to identify and segment cells in IHC images [9]. Whether using techniques such as cluster analysis [10], random forests [9], or deep neural networks [11-13], the workflow is similar. First, initial segmentation is performed (often in the pre-treatment step) to find cells and agglomerates. Then, training data is fed through a classifier to extract the most important features. Repeating this process while keeping only the most important feature vectors establishes a model, which is finally used to classify new test data.\n",
    "\n",
    "We will start with cluster analysis and random forest classifiers (from the scikit-learn package), but likely will also employ traditional GVF or ellipse-fitting algorithms (which we will implement in python) to refine initial segmentation. If necessary, we also intend to look into convolutional neural networks implemented with the TensorFlow package.\n",
    "\n",
    "### References\n",
    "\n",
    "[1] M. Veta, P.J. Van Diest, R. Kornegoor, A. Huisman, M.A. Viergever, and J.P.W. Pluim. (<b>2013</b>) Automatic Nuclei Segmentation in H&E Stained Breast Cancer Histopathology Images. <i>PLoS One</i>, 8, 7.\n",
    "\n",
    "[2] X. Yang, H. Li, and X. Zhou. (<b>2006</b>) Nuclei Segmentation using Marker-Controlled Watershed, Tracking using Mean-Shift, and Kalman Filter in Time-Lapse Microscopy. <i>IEEE Trans. Circuits Syst. I, Reg. Papers.</i>, 53, 11.\n",
    "\n",
    "[3] J. Cheng and J.C. Rajapakse. (<b>2009</b>) Segmentation of Clustered Nuclei with Shape Markers and Marking Function. <i>IEEE Trans. Biomed. Eng.</i>, 56, 3.\n",
    "\n",
    "[4] S. Ali and A. Madabhushi, “An Integrated Region-, Boundary-, Shape-Based Active Contour for Multiple Object Overlap Resolution in Histological Imagery. <i>IEEE Trans. Med. Imag.</i>, 31, 7.\n",
    "\n",
    "[5] S. Kothari, Q. Chaudry, M.D. Wang. (2009) Automated Cell Counting and Cluster Segmentation Using Concavity Detection and Ellipse Fitting Techniques. <i>Proc. IEEE Int. Symp. Biomed. Imaging</i>, 795.\n",
    "\n",
    "[6] Y. Toyoshima, T. Tokunaga, O. Hirose, M. Kanamori, T. Teramoto, M.S. Jang, S. Kuge, T. \n",
    "Ishihara, R. Yoshida, and Y. Iino. (<b>2006</b>) Accurate Automatic Detection of Densely Distributed Cell Nuclei in 3D Space. <i>PLoS. Comput. Biol.</i>, 12, 6.\n",
    "\n",
    "[7] H.S. Wu, J. Gil, and J.Barba. (<b>1998</b>) Optimal Segmentation of Cell Images. <i>IEE P-Vis. Image. Sign.</i>, 145, 1.\n",
    "\n",
    "[8] S.F. Yang-Mao, Y.K. Chan, and Y.P. Chu. (<b>2008</b>) Edge enhancement nucleus and cytoplast contour detector of cervical smear images. <i>IEEE Trans. Syst. Man, Cybern. B</i>, 38, 2.\n",
    "\n",
    "[9] O. Rujuta and A.J. Vyavahare. (<b>2017</b>) Review of Nuclei Detection, Segmentation in Microscopic Images. <i>J. Bioengineer. Biomed. Sci.</i>, 7, 2.\n",
    "\n",
    "[10] S. Wienert, D. Heim, K. Saeger, A. Stenzinger, M. Beil, P. Hufnagl, M. Dietel, C. Denkert, F. Klauschen. (<b>2012</b>) Detection and Segmentation of Cell Nuclei in Virtual Microscopy Images: A Minimum-Model Approach. <i>Sci. Rep.</i>, 2, 503.\n",
    "\n",
    "[11] S.K. Sadanandan, P. Ranefall, S. Le Guyader, and C. Wahlby. (<b>2017</b>) Automated Training of Deep Convolutional Neural Networks for Cell Segmentation. <i>Sci. Rep.</i>, 7, 1.\n",
    "\n",
    "[12] K. Sirinukunwattana, S.E.A. Raza, Y.W Tsang, I.A. Cree, D.R.J. Snead, and N.M. Rajpoo. (<b>2016</b>) Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images. <i>IEEE Trans. Med. Imag.</i>, 35, 99.\n",
    "\n",
    "[13] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, and A. Sethi. (<b>2017</b>) A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology. <i>IEEE Trans. Med. Imag.</i>, 36, 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## change the following to where you have stored and/or extracted the files\n",
    "# path = './project/'\n",
    "path = '/Users/arrakis/Dropbox/Tool - Classes/ChBE 8803/Project'\n",
    "\n",
    "n_samples = 3\n",
    "#n_samples = 560 # 560 samples to train on\n",
    "\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python==3.2.0.6 in /Users/arrakis/anaconda3/lib/python3.6/site-packages\r\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/arrakis/anaconda3/lib/python3.6/site-packages (from opencv-python==3.2.0.6)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install opencv-python==3.2.0.6 # for mac\n",
    "# >> https://stackoverflow.com/questions/47963386/image-not-found-error-after-installing-opencv-python-wheel-on-mac\n",
    "\n",
    "## load all packages used below\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import threshold_otsu\n",
    "import zipfile, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import sklearn, cv2\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "from skimage import feature\n",
    "from skimage.filters import sobel\n",
    "from skimage.morphology import watershed\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ideas for improving on workflow\n",
    "\n",
    "* normalize/reshape images and/or bias to have fewer points being compared\n",
    "* do dimensional reduction / PCA on all training set images to parse into different cell/conditions type, then threshold each one with best method for group\n",
    "    - related to that, try different thresholding (Watershed, etc)\n",
    "* change up the regression technique and/or training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Images and Masks from Training Set\n",
    "\n",
    "* load_zipped_img takes an image and set of masks from the .zip\n",
    "* A working example of extracting an individual image and associated masks is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STEP 1: Load an image (by index) and corresponding masks from ZIPPED stage1_train as np array\n",
    "\n",
    "def load_zipped_img(path, img_index): # load an image and all its masks\n",
    "    z = zipfile.ZipFile(path,'r') # access zip folder\n",
    "    zlist = z.namelist() # list of files in zip folder\n",
    "    img_name = zlist[img_index] # get selected image\n",
    "    img_name = img_name[0:-1] # eliminate \"/\"\n",
    "    \n",
    "    # get image and return as np array\n",
    "    img_raw = z.read('{}/images/{}.png'.format(img_name,img_name)) # get raw image\n",
    "    img = io.BytesIO(img_raw) # convert image\n",
    "    img = mpimg.imread(img) # numpy array\n",
    "    img = np.flip(img,0) # flip image\n",
    "    \n",
    "    # get all masks and return as np array\n",
    "    mask_list = []\n",
    "    for string in zlist:\n",
    "        if string.startswith(img_name+'/mask'):\n",
    "            mask_list.append(string)\n",
    "    mask_list = mask_list[1:-1] # list of masks\n",
    "    \n",
    "    masks = []\n",
    "    for m in mask_list:\n",
    "        mask_raw = z.read(m) # get raw mask\n",
    "        mask = io.BytesIO(mask_raw) # convert mask\n",
    "        mask = mpimg.imread(mask) # numpy array\n",
    "        mask = np.flip(mask,0) # flip mask\n",
    "        masks.append(mask)\n",
    "        \n",
    "    return img, masks\n",
    "\n",
    "# WORKING EXAMPLE OF load_zipped_img\n",
    "# (img, masks) = load_zipped_img(path+'/stage1_train.zip',1)\n",
    "# imgplot = plt.imshow(img, origin='lower')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Segmentation\n",
    "\n",
    "* The grayscale function converts images from rgb to grayscale\n",
    "* The otsu function selects an optimal threshold for equal inter-/intra-class variance\n",
    "* Watershed segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADyZJREFUeJzt3VusXFd9x/Hvr8EYNQSBG5I6jlUHZKSaPpjICpFSIapI\nhFiVDA9UyQNxUVTzkKggUakBHshLJFpxEagoklEinIoSIi6KH9KGxKJCSE3AiUwuuCmmuMTYskGN\nICqSmzj/Psw+ZeJ1LnPO3I+/H+lo9qzZM/M/68z+nbXX3jOTqkKS+v3etAuQNHsMBkkNg0FSw2CQ\n1DAYJDUMBkkNg0FSw2CQ1DAYJDVeM+0CAF6bjfU6Lp52GdK69iIv/Kqq3jzIuisGQ5KtwH3AHwKv\nAPur6gtJ7gT+Cvhlt+onquqh7j4fB24FzgF/XVUPL/ccr+Ni3pnrB6lX0ho9Wt/4r0HXHWTE8DLw\nsap6MsklwBNJHulu+3xVfaZ/5SQ7gJuAtwNXAI8meVtVnRu0KEnTteIcQ1Wdqqonu+UXgaPAlmXu\nsge4v6rOVtXPgGPANaMoVtJkrGryMck24B3A413T7UmeSnJvkjd1bVuA5/vudoLlg0TSjBk4GJK8\nHvgm8NGq+g1wN/BWYCdwCvjswqqL3L15b3eSfUkOJzn8EmdXXbik8RkoGJJsoBcKX62qbwFU1emq\nOldVrwBf5ne7CyeArX13vxI4ef5jVtX+qtpVVbs2sHGY30HSiK0YDEkC3AMcrarP9bVv7lvt/cAz\n3fJB4KYkG5NcBWwHfjC6kiWN2yBHJa4DPgg8neRI1/YJ4OYkO+ntJhwHPgxQVc8meQD4Mb0jGrd5\nREKaLysGQ1V9n8XnDR5a5j53AXcNUZekKfKUaEkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ1DAYJDUM\nBkkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ1DAY\nJDUMBkkNg0FSw2CQ1DAYJDUMBkmNFb/tWpo3D588MvbnuOGKnWN/jmlyxKB1ZRKhsPA8k3quaTAY\ntG6s5w110gwGaQjrNYwMBmlI6zEcVgyGJFuTfDfJ0STPJvlI174pySNJftJdvqlrT5IvJjmW5Kkk\nV4/7l5A0WoOMGF4GPlZVfwxcC9yWZAdwB3CoqrYDh7rrADcC27uffcDdI69a0litGAxVdaqqnuyW\nXwSOAluAPcCBbrUDwPu65T3AfdXzGPDGJJtHXrk0Q9bb7sSq5hiSbAPeATwOXF5Vp6AXHsBl3Wpb\ngOf77naia5M0JwYOhiSvB74JfLSqfrPcqou01SKPty/J4SSHX+LsoGVImoCBgiHJBnqh8NWq+lbX\nfHphF6G7PNO1nwC29t39SuDk+Y9ZVfuraldV7drAxrXWL2kMBjkqEeAe4GhVfa7vpoPA3m55L/Bg\nX/st3dGJa4FfL+xySJoPg7xX4jrgg8DTSRZmWD4BfBp4IMmtwM+BD3S3PQTsBo4BvwU+NNKK+4x7\nwme9nw8vLWXFYKiq77P4vAHA9YusX8BtQ9a1rEmeD79Whorm2dyd+Tgvh4XmpU5pMXMVDPO2sa33\nd+Bp/ZqbYJjnDWyea9eFaS6CYT1sWOvhd9CFY+aDYT1tUOvpd9H6NvPBIGnyDAZJjZkOBofe0nTM\ndDBImg6DQVLDYJDU8AtnZsA45lJ8r4aGYTBM0TgnV/sf25DQarkrMQWTfg+FR3e0WgaD1g1HRqPj\nrsQFwnkMrYbBoDVbCJtZCogbrtg5tV2ntT7vLPXfAnclNLRZm8OYxQ1tObPWf2AwaERm7cVtOAzH\nYNDIzNqL+4Yrds5dQMwKg0EjNWvhoLVx8lHr3mpGDQZbjyMGSQ1HDBq5cf3Xdb5gcgwGzY2VAsfg\nGB2DQZoR5wffNIPOYNC6sdiIYp5HEQ+fPDK1+p18lGbYtL7NzGCQ5sCkw2Gmg2Geh4GLWW+/j9av\nmQ6G9cRQ0DyZ+WBwg5Imb+aDAeY/HOa9fl14VgyGJPcmOZPkmb62O5P8IsmR7md3320fT3IsyXNJ\nbhhX4fPAd/dpXg1yHsNXgH8A7juv/fNV9Zn+hiQ7gJuAtwNXAI8meVtVnRu20MU2MD+uTBqPFYOh\nqr6XZNuAj7cHuL+qzgI/S3IMuAb4tzVXuAw3Ymk8hjnz8fYktwCHgY9V1QvAFuCxvnVOdG3SVCw3\nqvQfy9LWOvl4N/BWYCdwCvhs155F1q3FHiDJviSHkxx+ibNrLENaOz97YWlrCoaqOl1V56rqFeDL\n9HYXoDdC2Nq36pXAySUeY39V7aqqXRvYuJYypKFN65TjWbemYEiyue/q+4GFIxYHgZuSbExyFbAd\n+MFwJa4/DmFnj+HwaivOMST5GvBu4NIkJ4BPAe9OspPebsJx4MMAVfVskgeAHwMvA7eN4oiEpMka\n5KjEzYs037PM+ncBdw1T1IVgml+MIq3Ez2OYonHsUhg2GoW5OCVag3P+QqNgMKxDhoOGZTCsU4aD\nhmEwrGOGg9bKycc55kTj6C0XphdSfztimFMX0otUk2cwSGoYDJIaBoOkhsEgqWEwSHNg0oeeDQZp\nQBfSeSEGgzTjphFIBoO0CpPeSP22a0mvMs1dF4NBWqVxb7Cz8EVFvldCWoNpb7jj5ohBUsNgkNQw\nGCQ1DAZJDYNBUsOjErqgrfejC2vliEEXLENhaY4YNPfcwEfPEYPmmqEwHo4YNJBBN8BJfkitoTA+\nBoNGqn9jHUdIGAaTYTBobNyI55dzDJIaBoOkhsEgqWEwzCn33zVOK04+JrkX+HPgTFX9Sde2Cfg6\nsA04DvxFVb2QJMAXgN3Ab4G/rKonx1O6zg8Hv89SozLIiOErwHvPa7sDOFRV24FD3XWAG4Ht3c8+\n4O7RlKlBOIrQqKw4Yqiq7yXZdl7zHuDd3fIB4F+Bv+3a76uqAh5L8sYkm6vq1KgK1vIMB43CWucY\nLl/Y2LvLy7r2LcDzfeud6NoaSfYlOZzk8EucXWMZksZh1JOPWaStFluxqvZX1a6q2rWBjSMuQ9Iw\n1hoMp5NsBuguz3TtJ4CtfetdCZxce3mSpmGtwXAQ2Nst7wUe7Gu/JT3XAr92fkGaP4McrvwavYnG\nS5OcAD4FfBp4IMmtwM+BD3SrP0TvUOUxeocrPzSGmiWN2SBHJW5e4qbrF1m3gNuGLUrSdHnmo6SG\nwSCpYTBIahgMkhoGg6SGwSCpYTBIahgMkhp+SrSGNo0PiPHt5T2r6fuLNg/+uI4YtGYPnzwytU+N\n8tOqxtsHBoPWxA1zusbd/waDVm1WQmFW6pi0SfzeBoOkhsGgVblQ/0tfaAwGSQ2DQVLDYJDUMBgk\nNQwGSQ2DQVLDYJDUMBgkNQwGSQ2DQVLDYJDUMBgkNQwGSQ2DQVLDYJDUMBgkNQwGSQ2DQVLDYJDU\nGOoLZ5IcB14EzgEvV9WuJJuArwPbgOPAX1TVC8OVKWmSRjFi+LOq2llVu7rrdwCHqmo7cKi7Lo2c\n30Y1PuPYldgDHOiWDwDvG8NzSBqjYYOhgO8keSLJvq7t8qo6BdBdXjbkc2iGzMp/6VmpY70a9ktt\nr6uqk0kuAx5J8u+D3rELkn0Ar+P3hyxDk3TDFTun9v0SBsJkDBUMVXWyuzyT5NvANcDpJJur6lSS\nzcCZJe67H9gP8IZsqmHq0PpnIEzWmnclklyc5JKFZeA9wDPAQWBvt9pe4MFhi9TsmeSGaihM3jAj\nhsuBbydZeJx/qqp/SfJD4IEktwI/Bz4wfJmaRQsb7Lh2KwyExU1iVy5V0x/FvyGb6p25ftplSHNl\nteFw0eZjT/SdVrCsYScfJU3J6kdUxwZe01OiJTUMBkkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ1DAY\nJDUMBkkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ\n1DAYJDUMBkkNg0FSw2CQ1DAYJDUMBkkNg0FSw2CQ1DAYJDXGFgxJ3pvkuSTHktwxrueRNHpjCYYk\nFwFfAm4EdgA3J9kxjueSNHrjGjFcAxyrqv+sqv8F7gf2jOm5JI3YuIJhC/B83/UTXdv/S7IvyeEk\nh1/i7JjKkLQWrxnT42aRtnrVlar9wH6AJL98tL7xP8CvxlTPKF3KfNQJ81OrdY7eYrX+0aB3Hlcw\nnAC29l2/Eji51MpV9eYkh6tq15jqGZl5qRPmp1brHL1hax3XrsQPge1JrkryWuAm4OCYnkvSiI1l\nxFBVLye5HXgYuAi4t6qeHcdzSRq9ce1KUFUPAQ+t4i77x1XLiM1LnTA/tVrn6A1Va6pq5bUkXVA8\nJVpSY+rBMOunTic5nuTpJEeSHO7aNiV5JMlPuss3TaGue5OcSfJMX9uidaXni10fP5Xk6hmo9c4k\nv+j69UiS3X23fbyr9bkkN0ywzq1JvpvkaJJnk3yka5+pfl2mztH1aVVN7YfexORPgbcArwV+BOyY\nZk2L1HgcuPS8tr8H7uiW7wD+bgp1vQu4GnhmpbqA3cA/0zu/5Frg8Rmo9U7gbxZZd0f3OtgIXNW9\nPi6aUJ2bgau75UuA/+jqmal+XabOkfXptEcM83rq9B7gQLd8AHjfpAuoqu8B/31e81J17QHuq57H\ngDcm2TyZSpesdSl7gPur6mxV/Qw4Ru91MnZVdaqqnuyWXwSO0jtjd6b6dZk6l7LqPp12MKx46vQM\nKOA7SZ5Isq9ru7yqTkHvjwRcNrXqXm2puma1n2/vhuD39u2OzUStSbYB7wAeZ4b79bw6YUR9Ou1g\nWPHU6RlwXVVdTe+dorclede0C1qDWeznu4G3AjuBU8Bnu/ap15rk9cA3gY9W1W+WW3WRtonVukid\nI+vTaQfDqk6dnoaqOtldngG+TW8IdnphyNhdnpleha+yVF0z189VdbqqzlXVK8CX+d3Qdqq1JtlA\nb2P7alV9q2ueuX5drM5R9um0g2GmT51OcnGSSxaWgfcAz9CrcW+32l7gwelU2FiqroPALd0s+rXA\nrxeGxtNy3r74++n1K/RqvSnJxiRXAduBH0yopgD3AEer6nN9N81Uvy5V50j7dBKzqCvMsO6mN6v6\nU+CT067nvNreQm8290fAswv1AX8AHAJ+0l1umkJtX6M3XHyJ3n+EW5eqi95Q8ktdHz8N7JqBWv+x\nq+Wp7oW7uW/9T3a1PgfcOME6/5TeEPsp4Ej3s3vW+nWZOkfWp575KKkx7V0JSTPIYJDUMBgkNQwG\nSQ2DQVLDYJDUMBgkNQwGSY3/A36KWr+LQFXSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1293f7f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## STEP 2: Grayscale image and segment objects by thresholding images\n",
    "\n",
    "# convert to grayscale\n",
    "def grayscale(im):\n",
    "    return rgb2gray(im)\n",
    "\n",
    "# Otsu's Method, calculates optimal threshold for equal inter-/intra-class variance\n",
    "def otsu(image_gray):\n",
    "    threshold_val = threshold_otsu(image_gray) #Select threshold from Otsu's method\n",
    "    img_masked = np.where(image_gray > threshold_val, 1, 0)\n",
    "\n",
    "    if np.sum(img_masked==0) < np.sum(img_masked==1):\n",
    "        img_masked = np.where(img_masked, 0, 1)\n",
    "    return img_masked\n",
    "\n",
    "# Function to convert float32 raw images to int8 single channel\n",
    "def float2int8(img_float32):\n",
    "##\n",
    "    max_8bit = 255.0 # depends on dtype of image data\n",
    "    max_16bit = 65535.0\n",
    "    phi = 1\n",
    "    theta = 1\n",
    "  # increase intensity s.t. dark pixels become much brighter, bright pixels become slightly bright\n",
    "    intensified_float32 = (max_8bit/phi)*(img_float32/(max_8bit/theta))**0.5\n",
    "  # convert to int16 format for histogram equalization\n",
    "    img_int16 = plt.array(intensified_float32, dtype=plt.uint16)\n",
    "  # CLAHE (contrast limited adaptive histogram equalization)\n",
    "  # >> https://docs.opencv.org/3.3.0/d3/dc1/tutorial_basic_linear_transform.html\n",
    "    clahe = cv2.createCLAHE(clipLimit=50.0, tileGridSize=(20,20))\n",
    "    img_clahe = clahe.apply(img_int16)\n",
    "  # convert to int8 for threshold and watershed\n",
    "    img_int8 = cv2.convertScaleAbs(img_clahe, alpha=(max_8bit/max_16bit))\n",
    "    return img_int8\n",
    "\n",
    "# Function to watershed segment images\n",
    "def watershed(img_float32):\n",
    "# >> http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_watershed/py_watershed.html\n",
    "# >> http://scikit-image.org/docs/dev/user_guide/tutorial_segmentation.html\n",
    "##\n",
    "  # convert input image (float32) to 3-channel int8\n",
    "    img3 = cv2.cvtColor(img_float32, cv2.COLOR_RGB2BGR)\n",
    "    ch1,ch2,ch3 = cv2.split(img3)\n",
    "    ret,thresh1 = cv2.threshold(float2int8(ch1),0,255,cv2.THRESH_OTSU)\n",
    "    ret,thresh2 = cv2.threshold(float2int8(ch2),0,255,cv2.THRESH_OTSU)\n",
    "    ret,thresh3 = cv2.threshold(float2int8(ch3),0,255,cv2.THRESH_OTSU)\n",
    "    img_guess = cv2.merge((thresh1, thresh2, thresh3))\n",
    "  \n",
    "  # converge input image to 1-channel int8 grayscale and threshold (Otsu)\n",
    "    img_grey = grayscale(img_float32)\n",
    "    int8_grey = float2int8(img_grey)\n",
    "    ret, int8_thresh = cv2.threshold(int8_grey,0,255,cv2.THRESH_OTSU)\n",
    "  # noise removal\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    opening = cv2.morphologyEx(int8_thresh,cv2.MORPH_OPEN,kernel,iterations=2)\n",
    "  # sure background area\n",
    "    sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
    "    reduced_area = cv2.dilate(opening,kernel,iterations=10)\n",
    "  # finding sure foreground area\n",
    "    dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform,0.2*dist_transform.max(),255,0)\n",
    "  # finding unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg,sure_fg)\n",
    "  # marker labelling\n",
    "    ret, markers = cv2.connectedComponents(sure_fg)\n",
    "    markers = markers+1 # add one to all labels so sure background is 1 (not 0)\n",
    "    markers[unknown==255] = 0 # mark unknown region as 0\n",
    "  # apply watershed and mark boundary as -1\n",
    "    markers = cv2.watershed(img_guess, markers)\n",
    "    img_guess[markers == -1] = [255,0,0]\n",
    "\n",
    "    return img_guess, markers, sure_bg, sure_fg, unknown, reduced_area\n",
    "\n",
    "\n",
    "#### TESTING\n",
    "(img, mask) = load_zipped_img(path+'/stage1_train.zip', 5)\n",
    "\n",
    "img3 = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "ch1,ch2,ch3 = cv2.split(img3)\n",
    "ret,thresh1 = cv2.threshold(float2int8(ch1),0,255,cv2.THRESH_OTSU)\n",
    "ret,thresh2 = cv2.threshold(float2int8(ch2),0,255,cv2.THRESH_OTSU)\n",
    "ret,thresh3 = cv2.threshold(float2int8(ch3),0,255,cv2.THRESH_OTSU)\n",
    "img_guess = cv2.merge((thresh1, thresh2, thresh3))\n",
    "\n",
    "img_grey = grayscale(img)\n",
    "int8_grey = float2int8(img_grey)\n",
    "ret, int8_thresh = cv2.threshold(int8_grey,0,255,cv2.THRESH_OTSU)\n",
    "\n",
    "# noise removal\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "opening = cv2.morphologyEx(int8_thresh,cv2.MORPH_OPEN,kernel,iterations=2)\n",
    "\n",
    "# sure background area\n",
    "sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
    "reduced_area = cv2.dilate(opening,kernel,iterations=10)\n",
    "\n",
    "# finding sure foreground area\n",
    "dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "ret, sure_fg = cv2.threshold(dist_transform,0.2*dist_transform.max(),255,0)\n",
    "\n",
    "# finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg,sure_fg)\n",
    "\n",
    "# marker labelling\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "markers = markers+1 # add one to all labels so sure background is 1 (not 0)\n",
    "markers[unknown==255] = 0 # mark unknown region as 0\n",
    "\n",
    "# apply watershed and mark boundary as -1\n",
    "markers = cv2.watershed(img_guess, markers)\n",
    "img_guess[markers == -1] = [255,0,0]\n",
    "\n",
    "imgplot = plt.imshow(reduced_area, origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Individual Objects\n",
    "\n",
    "* The function separate_obj separates the objects in an image after a thresholding method has been applied\n",
    "* The function convert2runlength finds the objects in an image (1 corresponds to object, 0 to backgroun) and finds runs of continuous object pixels\n",
    "* The function rle generates a dataframe of images in run-length format. This is the output format required by the Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STEP 3: Separate individual objects and encode in run-length format\n",
    "\n",
    "# separate objects in image into individual masks\n",
    "def separate_obj(img_masked):\n",
    "    labels, nlabels = ndimage.label(img_masked)\n",
    "\n",
    "    label_arrays = []\n",
    "    for label_num in range(1, nlabels+1):\n",
    "        label_mask = np.where(labels == label_num, 1, 0)\n",
    "        label_arrays.append(label_mask)\n",
    "    return labels, nlabels, label_mask\n",
    "\n",
    "# convert path to run-length encoding (RLE) output format\n",
    "def convert2runlength(x):\n",
    "    obj = np.where(x.T.flatten()==1)[0] #1 corresponds to object, 0 to background\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in obj: # find continuous set of object pixels\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return \" \".join([str(i) for i in run_lengths])\n",
    "\n",
    "def rle(img_masked, im_id):\n",
    "    (labels, nlabels, label_mask) = separate_obj(img_masked)\n",
    "    im_df = pd.DataFrame()\n",
    "    for label_num in range(1, nlabels+1):\n",
    "        label_mask = np.where(labels == label_num, 1, 0)\n",
    "        if label_mask.flatten().sum() > 10:\n",
    "            rle = convert2runlength(label_mask)\n",
    "            s = pd.Series({'ImageId': im_id, 'EncodedPixels': rle})\n",
    "            im_df = im_df.append(s, ignore_index=True)\n",
    "    return im_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Shape Manipulation\n",
    "\n",
    "* The function one_index takes an image (2d array) and converts it to a 1d array. They are indexed from top to bottom then left to right\n",
    "* The function pad_normalize helps account for variation in image sizes. It determines the maximum length in a set of one-indexed images and \"pads\" all other one-indexed images with zeros so that all images have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-indexes a 2d array into 1d, top down then left right, output is np 1d array\n",
    "def one_index(arr2d):\n",
    "    h, w = arr2d.shape[0:2]\n",
    "    \n",
    "    arr1d = []\n",
    "    for col in range(0, w):\n",
    "        for row in range(0, h):\n",
    "            arr1d.append(arr2d[row][col])\n",
    "    return np.array(arr1d)\n",
    "    \n",
    "# pads all vectors in array to have max_len, returns np array\n",
    "def pad_normalize(array, max_len):\n",
    "    for i in range(0, len(array)):\n",
    "        vec = array[i]\n",
    "        if len(vec) < max_len:\n",
    "            array[i] = np.concatenate(( np.array(vec).reshape(1,-1), np.zeros((1, (max_len-len(vec)))) ), axis=1)\n",
    "        else:\n",
    "            array[i] = np.array(vec).reshape(1,-1)\n",
    "    return np.array(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model\n",
    "\n",
    "* X = a vector of one-indexed images that have been thresholded (The nuclei predicted by a thresholding method)\n",
    "* Y = a vector containing the sum of the one-indexed masks for each image (The correct nuclei)\n",
    "* Fits a model to find nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXAMPLE TRAINING\n",
    "\n",
    "z = zipfile.ZipFile(path+'/stage1_train.zip','r') # access zip folder\n",
    "zlist = z.namelist() # list of files in zip directory\n",
    "samples = zlist[0:n_samples-1] # 0 < samples <= 560\n",
    "\n",
    "x_train = [] # predicted segmentation using Otsu's thresholding\n",
    "y_train = [] # \"correct\" segmentation from sum of masks\n",
    "max_len = 0\n",
    "for i in range(0, n_samples):\n",
    "    (img, masks) = load_zipped_img(path+'/stage1_train.zip', i) # loads image and associated masks\n",
    "    m,n = img.shape[0:2]\n",
    "    \n",
    "    x_raw = otsu(grayscale(img))\n",
    "    y_raw = sum(masks)\n",
    "    x = np.resize(x_raw, (200, 200))\n",
    "    y = np.resize(y_raw, (100, 100))\n",
    "    x_vec = one_index(x)\n",
    "    y_vec = one_index(y)\n",
    "    print(x_vec.shape)\n",
    "    \n",
    "    imgplot = plt.imshow(x, origin='lower')\n",
    "    plt.show()\n",
    "    \n",
    "    #imgplot = plt.imshow(y, origin='lower')\n",
    "    #plt.show()\n",
    "#     x_vec = one_index(otsu(grayscale(img)))\n",
    "#     y_vec = one_index(sum(masks))\n",
    "            \n",
    "    if len(x_vec) > max_len: max_len = len(x_vec)\n",
    "    x_train.append(x_vec)\n",
    "    y_train.append(y_vec)\n",
    "\n",
    "x_train = np.squeeze(pad_normalize(x_train, max_len), axis=1)\n",
    "x_shortfeature = x_train[:, 0:10000]\n",
    "y_train = np.squeeze(pad_normalize(y_train, max_len), axis=1)\n",
    "y_shortfeature = y_train[:, 0:10000]\n",
    "\n",
    "# linreg = linear_model.LinearRegression() # create linear regression object\n",
    "# linreg.fit(x_shortfeature, y_shortfeature) # train the model using the training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "* Tests the performance of the model\n",
    "* Determines the number of true positives, true negatives, false positives and false negatives (Will add?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## EXAMPLE TESTING\n",
    "\n",
    "# (img_test, mask_test) = load_zipped_img(path+'/stage1_train.zip', 5)\n",
    "# X_test = one_index(otsu(grayscale(img)))\n",
    "# X_test = X_test[:10000].reshape(1, -1)\n",
    "# y_pred = linreg.predict(X_test) # predict using the testing set\n",
    "# y_pred = sum(y_pred).reshape(1,-1)\n",
    "# y_pred = np.round(y_pred)\n",
    "\n",
    "# # Scoring\n",
    "# y_actual = one_index(sum(mask_test))[:10000].reshape(1, -1)\n",
    "# accuracy = sklearn.metrics.accuracy_score(y_pred.T,y_actual.T)\n",
    "\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
