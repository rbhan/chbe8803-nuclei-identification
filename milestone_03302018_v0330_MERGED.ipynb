{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Optimizing Machine Learning Techniques for Automatic Nuclei Detection\n",
    "\n",
    "<b>1. Motivation</b>\n",
    "\n",
    "Many people die from cancer every year. This is terribly sad. :’( We would like to prevent this. One option is to identify the cancer early; treatment of nascent cancers tends to be more successful. Often, cells suffering from cancer display different biomarkers in their nuclei. These biomarkers can be seen in immunohistochemical (IHC) imaging. If there were a rapid and accurate way to screen images for cancerous cells, many cancer prognoses could be improved. We aim to develop an automated cell nuclei detection technique.\n",
    "\n",
    "<b>2. Challenges</b>\n",
    "\n",
    "Imaging conditions can vary tremendously - illumination, contrast, fluorescence and staining will all affect the appearance of the cell. We would also like our technique to be generalizable to many cell types because cancer is insidious and affects many cell types. Depending on the biological sample that was imaged, cells may also be aggregated; where a trained eye could distinguish individual nuclei, an algorithm might falter. \n",
    "\n",
    "<b>3. Pre-treatment and Initial Segmentation (“Detection”)</b>\n",
    "\n",
    "Colour is usually either normalized or thresholded to remove noise and background [1]. A variety of pre-processing techniques can be used to find objects, from conventional blob detection, to morphological and/or contour resolution, to watershed segmentation [2-4]. We will use sensible discretion to pick a pre-treatment technique.\n",
    "\n",
    "<b>4. Segmentation and Identification</b>\n",
    "\n",
    "There are two primary approaches to nuclei segmentation:\n",
    "\n",
    "<b><i>Traditional methods segment nuclei from single or overlapping cells</i></b>\n",
    "\n",
    "Several traditional methods, often involving a-priori knowledge of cell shape and size [5]. First, cell clusters are segmented from the background by concavity [6-7]. Next, cell clusters can be separated into individual cells based on the concavity of the intensity distribution [7]. After individual cells have been identified, cell boundaries are often approximated using elliptical curve-fitting techniques [5,7]. Further segmentation can be applied to separate the cell nucleus from the cytoplasm. A gradient vector flow active contour model (GVF-ACM) has been shown to find boundaries between the nucleus and cytoplasm [8].\n",
    "\n",
    "In this work, we can begin performing segmentation using the scikit-image package for python. This package includes methods for ellipse and boundary fitting, as well as edge detection and active contour modeling.\n",
    "\n",
    "<b><i>Machine learning methods identify nuclei via classification algorithms</i></b>\n",
    "\n",
    "Machine learning and pattern recognition have been successfully used to identify and segment cells in IHC images [9]. Whether using techniques such as cluster analysis [10], random forests [9], or deep neural networks [11-13], the workflow is similar. First, initial segmentation is performed (often in the pre-treatment step) to find cells and agglomerates. Then, training data is fed through a classifier to extract the most important features. Repeating this process while keeping only the most important feature vectors establishes a model, which is finally used to classify new test data.\n",
    "\n",
    "We will start with cluster analysis and random forest classifiers (from the scikit-learn package), but likely will also employ traditional GVF or ellipse-fitting algorithms (which we will implement in python) to refine initial segmentation. If necessary, we also intend to look into convolutional neural networks implemented with the TensorFlow package.\n",
    "\n",
    "### References\n",
    "\n",
    "[1] M. Veta, P.J. Van Diest, R. Kornegoor, A. Huisman, M.A. Viergever, and J.P.W. Pluim. (<b>2013</b>) Automatic Nuclei Segmentation in H&E Stained Breast Cancer Histopathology Images. <i>PLoS One</i>, 8, 7.\n",
    "\n",
    "[2] X. Yang, H. Li, and X. Zhou. (<b>2006</b>) Nuclei Segmentation using Marker-Controlled Watershed, Tracking using Mean-Shift, and Kalman Filter in Time-Lapse Microscopy. <i>IEEE Trans. Circuits Syst. I, Reg. Papers.</i>, 53, 11.\n",
    "\n",
    "[3] J. Cheng and J.C. Rajapakse. (<b>2009</b>) Segmentation of Clustered Nuclei with Shape Markers and Marking Function. <i>IEEE Trans. Biomed. Eng.</i>, 56, 3.\n",
    "\n",
    "[4] S. Ali and A. Madabhushi, “An Integrated Region-, Boundary-, Shape-Based Active Contour for Multiple Object Overlap Resolution in Histological Imagery. <i>IEEE Trans. Med. Imag.</i>, 31, 7.\n",
    "\n",
    "[5] S. Kothari, Q. Chaudry, M.D. Wang. (2009) Automated Cell Counting and Cluster Segmentation Using Concavity Detection and Ellipse Fitting Techniques. <i>Proc. IEEE Int. Symp. Biomed. Imaging</i>, 795.\n",
    "\n",
    "[6] Y. Toyoshima, T. Tokunaga, O. Hirose, M. Kanamori, T. Teramoto, M.S. Jang, S. Kuge, T. \n",
    "Ishihara, R. Yoshida, and Y. Iino. (<b>2006</b>) Accurate Automatic Detection of Densely Distributed Cell Nuclei in 3D Space. <i>PLoS. Comput. Biol.</i>, 12, 6.\n",
    "\n",
    "[7] H.S. Wu, J. Gil, and J.Barba. (<b>1998</b>) Optimal Segmentation of Cell Images. <i>IEE P-Vis. Image. Sign.</i>, 145, 1.\n",
    "\n",
    "[8] S.F. Yang-Mao, Y.K. Chan, and Y.P. Chu. (<b>2008</b>) Edge enhancement nucleus and cytoplast contour detector of cervical smear images. <i>IEEE Trans. Syst. Man, Cybern. B</i>, 38, 2.\n",
    "\n",
    "[9] O. Rujuta and A.J. Vyavahare. (<b>2017</b>) Review of Nuclei Detection, Segmentation in Microscopic Images. <i>J. Bioengineer. Biomed. Sci.</i>, 7, 2.\n",
    "\n",
    "[10] S. Wienert, D. Heim, K. Saeger, A. Stenzinger, M. Beil, P. Hufnagl, M. Dietel, C. Denkert, F. Klauschen. (<b>2012</b>) Detection and Segmentation of Cell Nuclei in Virtual Microscopy Images: A Minimum-Model Approach. <i>Sci. Rep.</i>, 2, 503.\n",
    "\n",
    "[11] S.K. Sadanandan, P. Ranefall, S. Le Guyader, and C. Wahlby. (<b>2017</b>) Automated Training of Deep Convolutional Neural Networks for Cell Segmentation. <i>Sci. Rep.</i>, 7, 1.\n",
    "\n",
    "[12] K. Sirinukunwattana, S.E.A. Raza, Y.W Tsang, I.A. Cree, D.R.J. Snead, and N.M. Rajpoo. (<b>2016</b>) Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images. <i>IEEE Trans. Med. Imag.</i>, 35, 99.\n",
    "\n",
    "[13] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, and A. Sethi. (<b>2017</b>) A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology. <i>IEEE Trans. Med. Imag.</i>, 36, 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## change the following to where you have stored and/or extracted the files\n",
    "# path = './project/'\n",
    "path = '/Users/arrakis/Dropbox/Tool - Classes/ChBE 8803/Project'\n",
    "\n",
    "n_samples = 20 #665 # total datasets in train1\n",
    "\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPDATED 3/30 future work\n",
    "\n",
    "* output accuracy scores to .csv\n",
    "* add SVM and model compare\n",
    "* fix feature vector creation for RFC s.t. we don't do it 4 times per run\n",
    "* manually correct bad images -> in watershed as well\n",
    "* active contour ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Requirements\n",
    "\n",
    "* numpy version 1.13.3\n",
    "* pandas version 0.20.3\n",
    "* matplotlib version\n",
    "* sklearn version 0.19.1\n",
    "* skimage version 0.13.0\n",
    "* cv2 (used for image processing)\n",
    "    * See lines 1-3 in the next block for installation on mac (version 3.2.0.6)\n",
    "    * For PC, installation worked with pip install opencv-python from the anaconda prompt (version 3.4.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install opencv-python==3.2.0.6 # for mac\n",
    "# >> https://stackoverflow.com/questions/47963386/image-not-found-error-after-installing-opencv-python-wheel-on-mac\n",
    "\n",
    "## load all packages used below\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import threshold_otsu\n",
    "import zipfile, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import sklearn, cv2\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import label\n",
    "from skimage import feature\n",
    "from skimage.filters import sobel, laplace\n",
    "from skimage.morphology import watershed\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "* ```stage1_train.zip```: zipped training set of 665 images, each with between 4 and ~380 masks (total = 32145 files)\n",
    "* ```stage1_test.zip```: zipped test set of 65 images\n",
    "\n",
    "Because these datasets are so large, we use ```load_zipped_img``` to access the .zip file directly and read out images and associated masks without extracting the .zip into a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_img(path, img_index): # load an image and all its masks\n",
    "    z = zipfile.ZipFile(path,'r') # access zip folder\n",
    "    zlist = z.namelist() # list of files in zip folder\n",
    "    \n",
    "    img_name = zlist[img_index] # get selected image\n",
    "    img_name = img_name[0:-1] # eliminate \"/\"\n",
    "    \n",
    "    # get image and return as np array\n",
    "    img_raw = z.read('{}/images/{}.png'.format(img_name,img_name)) # get raw image\n",
    "    img = io.BytesIO(img_raw) # convert image\n",
    "    img = mpimg.imread(img) # numpy array\n",
    "    img = np.flip(img,0) # flip image\n",
    "    \n",
    "    # get all masks and return as np array\n",
    "    mask_list = []\n",
    "    for string in zlist:\n",
    "        if string.startswith(img_name+'/mask'):\n",
    "            mask_list.append(string)\n",
    "    mask_list = mask_list[1:-1] # list of masks\n",
    "    \n",
    "    masks = []\n",
    "    for m in mask_list:\n",
    "        mask_raw = z.read(m) # get raw mask\n",
    "        mask = io.BytesIO(mask_raw) # convert mask\n",
    "        mask = mpimg.imread(mask) # numpy array\n",
    "        mask = np.flip(mask,0) # flip mask\n",
    "        masks.append(mask)\n",
    "        \n",
    "    return img, masks\n",
    "\n",
    "# WORKING EXAMPLE OF load_zipped_img\n",
    "\n",
    "# (img, masks) = load_zipped_img(path+'/stage1_train.zip',177)\n",
    "\n",
    "# fig, ax = plt.subplots(1,2)\n",
    "\n",
    "# ax[0].imshow(sum(masks), origin='lower')\n",
    "# ax[1].imshow(img, origin='lower')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection: Separating Object from Background\n",
    "\n",
    "Before determining the positions of nuclei in an image, it is key to separate the image from the background using a process called image detection. This is typically done by thresholding - all pixels below some certain threshold are classified background (0) and all pixels above some certain threshold are classified object (1). We use Otsu's method to find the optimum threshold by searching exhaustively for a threshold such that inter-class variance is maximized (equivalent to saying that intra-class variance is minimized).\n",
    "\n",
    "* The grayscale function converts images from rgb to grayscale\n",
    "* The otsu function selects an optimal threshold for equal inter-/intra-class variance\n",
    "* The function float2int8 converts the data type to int8, which is required for the cv2 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to grayscale\n",
    "def grayscale(im):\n",
    "    return rgb2gray(im)\n",
    "\n",
    "# Otsu's method\n",
    "def otsu(image_gray):\n",
    "    threshold_val = threshold_otsu(image_gray) #Select threshold from Otsu's method\n",
    "    img_masked = np.where(image_gray > threshold_val, 1, 0)\n",
    "\n",
    "    if np.sum(img_masked==0) < np.sum(img_masked==1):\n",
    "        img_masked = np.where(img_masked, 0, 1)\n",
    "    return img_masked\n",
    "\n",
    "# Function to convert float32 raw images to int8 single channel\n",
    "def float2int8(img_float):\n",
    "    img_int8 = (img_float * 255).round().astype(np.uint8)\n",
    "    return img_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation: Separating Individual Objects\n",
    "\n",
    "After thresholding to binarize an image into object and background pixels, the next step is separating the object pixels into individual cells. We tried watershed segmentation and (active) contouring, two techniques commonly used in literature, and compared accuracy against a Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watershed algorithm\n",
    "\n",
    "Watershed segmentation treats a grayscale image like a topographic map. We flood the landscape by filling up the brightest areas first, and also preventing waters from different sources to mix. Together, these effectively define watershed lines (the edges of objects) and separate catchment basins (the objects we are trying to identify). The general algorithm is as follows\n",
    "* start with grayscale image\n",
    "* threshold (we used Otsu's method) to get an initial prediction of objects\n",
    "* remove noise via opening and/or Gaussian blurring (we used opening)\n",
    "* apply distance transform, which turns a binary image into gradient map (brightest areas are 1)\n",
    "* find sure background by expanding the borders of objects (dilation)\n",
    "* find sure foreground by shrinking the borders of objects (erosion) until they are no longer touching\n",
    "* flood the distance transformed image to recalculate new borders of objects as the watershed lines\n",
    "\n",
    "<u>Advantages</u>: watershed segmentation is intuitive, parallelizable, and the resulting boundaries always form closed and connected regions\n",
    "\n",
    "<u>Disadvantages</u>: watershed tends to over-segment, unable to capture cell overlap (because we prevent waters from merging) if the seeds (initial basins) are not well chosen\n",
    "\n",
    "```watershed``` takes an image and outputs\n",
    "* ```img_guess```: the binarized image guessed by watershed segmentation (watershed lines = cell outlines in red)\n",
    "* ```markers```: separated cells identified by watershed segmentation (the catchment basis)\n",
    "* ```sure_bg```: certain background pixels\n",
    "* ```sure_fg```: certain object pixels\n",
    "* ```uncertain```: the region between sure_bg and sure_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to watershed segment images\n",
    "def watershed(img_float32):\n",
    "    img = img_float32[:, :, :3]\n",
    "    \n",
    "    # convert input image (float32) to 3-channel int8\n",
    "    ch1 = float2int8(otsu(img[:,:,0:1][:,:,0]))\n",
    "    ch2 = float2int8(otsu(img[:,:,1:2][:,:,0]))\n",
    "    ch3 = float2int8(otsu(img[:,:,2:3][:,:,0]))\n",
    "    img_guess = cv2.merge([ch1,ch2,ch3])\n",
    "    \n",
    "    # greyscale and otsu threshold original image\n",
    "    img_grey = grayscale(img)\n",
    "    int8_thresh = float2int8(otsu(img_grey))\n",
    "\n",
    "    # noise removal\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    opening = cv2.morphologyEx(int8_thresh,cv2.MORPH_OPEN,kernel,iterations=2)\n",
    "    \n",
    "    # find sure background area\n",
    "    sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
    "    \n",
    "    # find sure foreground area\n",
    "    dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "    ret, sure_fg = cv2.threshold(dist_transform,0.2*dist_transform.max(),255,0)\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "\n",
    "    # finding uncertain region\n",
    "    uncertain = cv2.subtract(sure_bg,sure_fg)\n",
    "    \n",
    "    # marker labelling\n",
    "    ret, markers = cv2.connectedComponents(sure_fg)\n",
    "    markers = markers+1 # add one to all labels so sure background is 1 (not 0)\n",
    "    markers[uncertain==255] = 0 # mark unknown region as 0\n",
    "\n",
    "    # apply watershed and mark boundary as -1\n",
    "    markers = cv2.watershed(img_guess, markers)\n",
    "    img_guess[markers == -1] = [255,0,0]\n",
    "\n",
    "    return img_guess, markers, sure_bg, sure_fg, uncertain\n",
    "\n",
    "\n",
    "# WORKING EXAMPLE OF watershed\n",
    "\n",
    "# (img, mask) = load_zipped_img(path+'/stage1_train.zip', 510)\n",
    "\n",
    "# img_guess, markers, sure_bg, sure_fg, uncertain = watershed(img)\n",
    "\n",
    "# fig, ax = plt.subplots(1,3, figsize = (10,10))\n",
    "\n",
    "# ax[0].imshow(sum(mask), origin='lower')\n",
    "# ax[1].imshow(img, origin='lower')\n",
    "# ax[2].imshow(grayscale(img_guess), origin='lower')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge detection algorithm\n",
    "\n",
    "The interface between two contiguous homogeneous regions is usually a discontinuity in grey-level/colour, texture, or motion. These discontinuities can be located using edge detection algorithms that look for local gradient maxima\n",
    "$$\\nabla f = \\begin{bmatrix} G_x \\\\ G_y \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n",
    "\n",
    "<b>Simple edge detection</b><br>\n",
    "Detect cells as ellipses.\n",
    "\n",
    "<b>Active contour method (\"greedy snake\")</b><br>\n",
    "A snake (contour) is an elastic, deformable, energy-minimizing spline defined by $n$ points ${v} _{i}$ where $i=0 \\ldots n-1$. Joint level set optimization, essentially a fancy gradient descent method, is often used to energy minimize the snake's Hamiltonian\n",
    "$$ E_{snake} = \\int E_{snake} ds = \\int (E_{internal} + E_{image} + E_{constraints} ) ds $$\n",
    "The internal energy of the snake depends on the continuity and curvature of the contour, and the constraint energy allows initial points to be placed and the subsequently evolved to guide the snake towards or away from features in the image. The external energy is most commonly modified; there are many derivative methods and ways to process the features in the image, or even the image itself. Usually an image can be broken down into (weighted) functionals that describe the edges, lines, and terminations.\n",
    "\n",
    "<u>Advantages</u>: autonomously and adaptively searches for minimum state, external forces are intuitive, good at tracking dynamic objects, can be scale invariant\n",
    "\n",
    "<u>Disadvantages</u>: sensitive to local minima states, often miss small/sutle features in the image, accuracy and speed depend on convergence algorithm\n",
    "\n",
    "```contouring``` takes an image and outputs\n",
    "* ```img_contouring```: the raw image with circular and rectangle contours detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to contour segment images\n",
    "def contouring(img_float32, borderSize, gap, draw='off'):  \n",
    "    img = float2int8(img_float32[:, :, :3])\n",
    "\n",
    "    # perform a BGR->HSV conversion and use the V channel for processing\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # threshold, apply morphological closing, then take the distance transform (dist)\n",
    "    th, bw = cv2.threshold(float2int8( hsv[:, :, 2] ), 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    morph = cv2.morphologyEx(bw, cv2.MORPH_CLOSE, kernel)\n",
    "    dist = cv2.distanceTransform(morph, cv2.DIST_L2, cv2.DIST_MASK_PRECISE)\n",
    "    distborder = cv2.copyMakeBorder(dist, borderSize, borderSize, borderSize, borderSize, cv2.BORDER_CONSTANT | cv2.BORDER_ISOLATED, 0)\n",
    "\n",
    "    # create a template, take its distance transform and use it as the template (temp)\n",
    "    kernel2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2*(borderSize-gap)+1, 2*(borderSize-gap)+1))\n",
    "    kernel2 = cv2.copyMakeBorder(kernel2, gap, gap, gap, gap, cv2.BORDER_CONSTANT | cv2.BORDER_ISOLATED, 0)\n",
    "    distTempl = cv2.distanceTransform(kernel2, cv2.DIST_L2, cv2.DIST_MASK_PRECISE)\n",
    "\n",
    "    # template matching (dist*temp)\n",
    "    nxcor = cv2.matchTemplate(distborder, distTempl, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "    # find local maxima of the resulting image, positions correspond to circle centers and values correspond to radii\n",
    "    mn, mx, _, _ = cv2.minMaxLoc(nxcor)\n",
    "\n",
    "    # thresholding template matched image\n",
    "    th, peaks = cv2.threshold(nxcor, mx*0.5, 255, cv2.THRESH_BINARY)\n",
    "    peaks8u = cv2.convertScaleAbs(peaks)\n",
    "    _, contours, _ = cv2.findContours(peaks8u, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # peaks8u = cv2.convertScaleAbs(peaks)    # to use as mask\n",
    "\n",
    "    # detecting circles as local maxima\n",
    "#     if draw == 'on':\n",
    "    for i in range(len(contours)):\n",
    "        x, y, w, h = cv2.boundingRect(contours[i])\n",
    "        _, mx, _, mxloc = cv2.minMaxLoc(dist[y:y+h, x:x+w], peaks8u[y:y+h, x:x+w])\n",
    "        cv2.circle(img, (int(mxloc[0]+x), int(mxloc[1]+y)), int(mx), (255, 0, 0), 2)\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "        cv2.drawContours(img, contours, i, (0, 0, 255), 2)\n",
    "            \n",
    "    img_contoured = img\n",
    "\n",
    "    return img_contoured\n",
    "\n",
    "# WORKING EXAMPLE OF contouring\n",
    "# (img, mask) = load_zipped_img(path+'/stage1_train.zip', 177)\n",
    "# img_guess, markers, sure_bg, sure_fg, uncertain = watershed(img)\n",
    "\n",
    "# # one way of determining borderSize and gap\n",
    "# num_markers = np.max(markers)\n",
    "# obj_size = []\n",
    "# for i in range(1, num_markers):\n",
    "#     obj_size.append(len(markers[markers==i]))\n",
    "\n",
    "# min_size = min(a for a in obj_size)\n",
    "# max_size = max(a for a in obj_size)\n",
    "# avg_size = np.sum(obj_size)/num_markers\n",
    "\n",
    "# borderSize = int(np.sqrt(5000))\n",
    "# gap = int(borderSize/10)\n",
    "\n",
    "# img_contoured = contouring(img, borderSize, gap, draw='off')\n",
    "\n",
    "# fig, ax = plt.subplots(1,3, figsize = (15,15))\n",
    "\n",
    "# ax[0].imshow(sum(mask), origin='lower')\n",
    "# ax[1].imshow(img_contoured, origin='lower')\n",
    "# ax[2].imshow(img_guess, origin='lower')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classification\n",
    "\n",
    "\n",
    "Random forest classifiers are ensemble learning methods that construct a \"forest\" of decision tree classifiers and return the mode of the individual trees. correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "<u>Advantages</u>: scale invariant, transformation invariant, robust to inclusion of irrelevant features, good accuracy (low variance) since they correct for decision trees' overfitting\n",
    "\n",
    "<u>Disadvantages</u>: less inspectable than decision trees\n",
    "\n",
    "X = feature vector (6 columns):\n",
    "* grayscale pixel intensity (continuous)\n",
    "* watershed prediction for a pixel (discrete)\n",
    "* magnitude of the Sobel gradient or Laplacian of pixel intensity (discrete)\n",
    "* intensity for each of the rgb colour channels (continuous)\n",
    "\n",
    "Y = a vector containing the sum of the one-indexed masks for each image (the correct nuclei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 56.5 ms, total: 1.78 s\n",
      "Wall time: 1.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.zeros(7).reshape(1,7) # Feature vector\n",
    "y_train = np.zeros(1) # True pixel values\n",
    "\n",
    "for i in range(0, n_samples):\n",
    "    (img, masks) = load_zipped_img(path+'/stage1_train.zip', i) # loads image and associated masks\n",
    "\n",
    "    (img_guess, markers, sure_bg, sure_fg, uncertain) = watershed(img)\n",
    "\n",
    "    intensity = grayscale(img)\n",
    "    img_guess = grayscale(img_guess)\n",
    "    \n",
    "    filter_sobel = sobel(grayscale(img))[uncertain==255].reshape(-1,1)\n",
    "    filter_laplace = laplace(grayscale(img))[uncertain==255].reshape(-1,1)\n",
    "    \n",
    "    intensity_raw = intensity[uncertain==255].reshape(-1,1)\n",
    "    watershed_predict = img_guess[uncertain==255].reshape(-1,1)\n",
    "    \n",
    "    img3 = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    r,g,b = cv2.split(img3)\n",
    "    \n",
    "    r = r[uncertain==255].reshape(-1,1)\n",
    "    g = g[uncertain==255].reshape(-1,1)\n",
    "    b = b[uncertain==255].reshape(-1,1)\n",
    "    \n",
    "    feature = np.concatenate((intensity_raw, watershed_predict, filter_sobel, filter_laplace, r, g, b), axis = 1)\n",
    "    y_raw = sum(masks)\n",
    "    y_raw = y_raw[uncertain==255].reshape(-1,1)\n",
    "    \n",
    "    x_train = np.append(x_train, feature).reshape(-1,7)\n",
    "    y_train = np.append(y_train, y_raw).reshape(-1,1)\n",
    "\n",
    "x_train = x_train[1:,:]\n",
    "y_train = y_train[1:,0]\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "%time clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "Quantify error of different models\n",
    "* Compute confusion matrix to find number of false positives (FP), false negatives (FN), true positive (TP), true negatives (TN)\n",
    "* Quantify error by calculating precision, accuracy, recall, and f1 score\n",
    "* Use cross-validation to tune hyperparameters\n",
    "\n",
    "<b>Detection accuracy metric</b><br>\n",
    "The fit target Y is sum of all true masks = true foreground\n",
    "* accuracy = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / (Total)\n",
    "* precision = (TP) / (TP + FP)\n",
    "* recall = (TP) / (TP + FN)\n",
    "* f1 = 2$*$(precision $*$ recall) / (precision + recall)\n",
    "\n",
    "<b>Segmentation accuracy metric</b><br>\n",
    "The fit target Y is individual masks for each nuclei. Intersection over union (IoU) of a proposed set of object pixels A and a set of true object pixels B is calculated as \n",
    "$IoU(A,B) = \\frac{A \\cap B}{A \\cup B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separates the objects in an image after a thresholding method has been applied\n",
    "def separate_obj(img_masked):\n",
    "    labels, nlabels = ndimage.label(img_masked)\n",
    "\n",
    "    label_arrays = []\n",
    "    for label_num in range(1, nlabels+1):\n",
    "        label_mask = np.where(labels == label_num, 1, 0)\n",
    "        label_arrays.append(label_mask)\n",
    "    return labels, nlabels, label_mask\n",
    "\n",
    "# separates a sum of masks into individual masks\n",
    "def predicted_mask_list(total_mask): \n",
    "    obj_detected = np.max(total_mask) + 1\n",
    "    mask_list_pred = []\n",
    "    for i in range(1, obj_detected):\n",
    "        mask_p = total_mask == i*np.ones(total_mask.shape)\n",
    "        mask_list_pred.append(mask_p)\n",
    "        \n",
    "    return mask_list_pred\n",
    "\n",
    "# determines intersection over union metric\n",
    "def iou(img, predicted_masks, real_masks): \n",
    "    mask_iou = []\n",
    "    mask_num = len(masks)\n",
    "    \n",
    "    pred_mask_num = len(predicted_masks)\n",
    "    for i in range(0, mask_num):\n",
    "        individual_mask = masks[i]\n",
    "        best_match = 0\n",
    "        nearest_mask = 0\n",
    "        for j in range(0, pred_mask_num):\n",
    "            match = np.sum(predicted_masks[j] == real_masks[i])\n",
    "            \n",
    "            if match > best_match:\n",
    "                nearest_mask = j\n",
    "                best_match = match\n",
    "       \n",
    "        m = real_masks[i]\n",
    "            \n",
    "        mask_i = np.sum(m[predicted_masks[nearest_mask] == 1])\n",
    "        mask_u = np.sum(predicted_masks[nearest_mask] == 1) + np.sum(m == 1) - mask_i\n",
    "        mask_iou.append(mask_i/mask_u)\n",
    "            \n",
    "    return mask_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "Average accuracy: watershed = 96.5095481964243% ; random forest = 96.94593349345611%\n",
      "Average precision: watershed = 91.68672162791184% ; random forest = 82.72161583181739%\n",
      "Average f1-score: watershed = 87.26178163206356% ; random forest = 85.61611241929876%\n",
      "Average IoU: watershed = 60.832615239609765% ; random forest = 57.54850661491695%\n"
     ]
    }
   ],
   "source": [
    "score_watershed = []\n",
    "score_rfc = []\n",
    "\n",
    "bad_list = []\n",
    "for i in range(0, n_samples):\n",
    "    (img, masks) = load_zipped_img(path+'/stage1_train.zip', i)\n",
    "\n",
    "    y_real = sum(masks).reshape(-1,1)\n",
    "    shap = grayscale(img).shape\n",
    "    \n",
    "    # watershed prediction\n",
    "    (img_guess, markers, sure_bg, sure_fg, uncertain) = watershed(img)\n",
    "    \n",
    "    # watershed score\n",
    "    yp_watershed = np.round(grayscale(img_guess)).reshape(-1,1)\n",
    "    acc_watershed = sklearn.metrics.accuracy_score(y_real, yp_watershed)\n",
    "    prec_watershed = sklearn.metrics.precision_score(y_real, yp_watershed)\n",
    "    f1_watershed = sklearn.metrics.f1_score(y_real, yp_watershed)\n",
    "    labels_watershed, nlabels_watershed, label_mask_watershed = separate_obj(yp_watershed.reshape(shap[0], shap[1]))\n",
    "    masks_watershed = predicted_mask_list(labels_watershed)\n",
    "    iou_watershed = iou(yp_watershed, masks_watershed, masks)\n",
    "    score_watershed.append([acc_watershed, prec_watershed, f1_watershed, np.mean(iou_watershed)])\n",
    "    \n",
    "    # random forest prediction\n",
    "    intensity = grayscale(img)\n",
    "    intensity_raw = intensity.reshape(-1,1)\n",
    "    watershed_raw = grayscale(img_guess).reshape(-1,1)\n",
    "    filter_sobel = sobel(grayscale(img)).reshape(-1,1)\n",
    "    filter_laplace = laplace(grayscale(img)).reshape(-1,1)\n",
    "    img3 = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    ch1,ch2,ch3 = cv2.split(img3)\n",
    "    ch1 = ch1.reshape(-1,1)\n",
    "    ch2 = ch2.reshape(-1,1)\n",
    "    ch3 = ch3.reshape(-1,1)\n",
    "    feature = np.concatenate((intensity_raw,watershed_raw, filter_sobel, filter_laplace, ch1, ch2, ch3), axis = 1)\n",
    "\n",
    "    # random forest score\n",
    "    yp_rfc = clf.predict(feature)\n",
    "    y_real = sum(masks).reshape(-1,1)\n",
    "    acc_rfc = sklearn.metrics.accuracy_score(y_real, yp_rfc)\n",
    "    prec_rfc = sklearn.metrics.precision_score(y_real, yp_rfc)\n",
    "    f1_rfc = sklearn.metrics.f1_score(y_real, yp_rfc)\n",
    "    labels_rfc, nlabels_rfc, label_mask_rfc = separate_obj(grayscale(yp_rfc.reshape(shap[0], shap[1])))\n",
    "    masks_rfc = predicted_mask_list(labels_rfc)\n",
    "    iou_rfc = iou(yp_rfc, masks_rfc, masks)\n",
    "    score_rfc.append([acc_rfc, prec_rfc, f1_rfc, np.mean(iou_rfc)])\n",
    "    \n",
    "    if (prec_watershed < 0.7) or (acc_watershed < 0.7):\n",
    "        bad_list.append(i)\n",
    "\n",
    "ws = np.array(score_watershed)\n",
    "rfc = np.array(score_rfc)\n",
    "print(bad_list)\n",
    "print('Average accuracy: watershed = {0}% ; random forest = {1}%'.format(np.mean(ws[:,0])*100, np.mean(rfc[:,0])*100))\n",
    "print('Average precision: watershed = {0}% ; random forest = {1}%'.format(np.mean(ws[:,1])*100, np.mean(rfc[:,1])*100))\n",
    "print('Average f1-score: watershed = {0}% ; random forest = {1}%'.format(np.mean(ws[:,2])*100, np.mean(rfc[:,2])*100))\n",
    "print('Average IoU: watershed = {0}% ; random forest = {1}%'.format(np.mean(ws[:,3])*100, np.mean(rfc[:,3])*100))\n",
    "# df_score = pd.DataFrame({'acc_watershed': ws[:,0], 'prec_watershed': ws[:,1], \n",
    "#                          'f1_watershed': ws[:,2], 'avgIoU_watershed': ws[:,3],\n",
    "#                          'acc_randomforest': rfc[:,0], 'prec_randomforest': score_rfc[:,1], \n",
    "#                          'f1_randomforest': rfc[:,2], 'avgIoU_randomforest': rfc[:,3],\n",
    "#                         }) \n",
    "# df_score.to_csv('summary_model_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Improvements: Some Worked Better than Others\n",
    "\n",
    "As seen above, we have high scores for detection but lower scores for image segmentation. A significant challenge is that imaging, staining, and sample preparation conditions can vary dramatically. There does not exist a \"general\" method capable of accurate nucleus segmentation across cell types and image types.\n",
    "\n",
    "We hypothesized that applying preliminary dimensional reduction to the training set could separate out \"classes\" of images. By asessing the fit for different types of thresholding/segmentation, we might be able to process each set of images individually. Unfortunately, PCA and SVM methods both required enormous amounts of time to train (the largest images have 1445320 pixels, giving us a dataset that is N = 665 by D = 1445320)\n",
    "\n",
    "Clustering was much faster, and indeed we were able to identify 6 classes of images. Ultimately, adding this further classification to the Random Forest classifier did not improve accuracy.\n",
    "\n",
    "However, visually inspecting the different classes allowed us to do post-training processing on images with low precision scores (precision tends to be more important because accuracy may be inflated if there is more background in the image; i.e. we get more background pixels correct). Typically, those are images where instead of having two primary levels of contrast, there are several levels corresponding to background, cytoplasm, and nucleus. In those cases, we found that adjusting the thresholding allowed to to obtain significantly improved accuracy.\n",
    "\n",
    "The adjusted training dataset was then used to re-train our Random Forest model, which was finally applied to predict the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "The k-means algorithms partitions observations (feature vectors) in the given data into $k$ mutually exclusive clusters such that each point belongs to the cluster with the nearest mean, which is measured by the Euclidean distance.\n",
    "\n",
    "<u>Advantages</u>: efficient, easy to implement, easy to interpret\n",
    "\n",
    "<u>Disadvantages</u>: different results with each run depending on the initial random assignments, does not ensure results has global minimum variance, needs to known how many clusters to find\n",
    "\n",
    "To choose a feature vector, we can look at the images in the training set and see immediately that they tend to differ based on both cell features (geometry, shape, etc) and colour. Therefore we train a k-means clustering algorithm on geometry features and colour features, then combine the resulting classes to get six (6) overall classes.\n",
    "\n",
    "Geometry feature vector:\n",
    "* max contour area value per image\n",
    "* mean contour area value per image\n",
    "* number of contours per image\n",
    "* average value of gray pixels per image\n",
    "* width per image\n",
    "* length per image\n",
    "\n",
    "Colour feature vector:\n",
    "* red per image\n",
    "* green per image\n",
    "* blue per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image information, apply clustering\n",
    "def get_image_info(samples):\n",
    "    # geometry features\n",
    "    max_cnt_area = [] # max contour area value per image   \n",
    "    avg_cnt_area = [] # mean contour area value per image\n",
    "    num_cnt = [] # how many contour areas per image\n",
    "    wid_list = [] #  width per image\n",
    "    len_list = [] #  length per image\n",
    "    # colour features\n",
    "    avg_grey = [] # average value of gray pixels per image\n",
    "    r = [] #  red per image\n",
    "    g = [] # green  \n",
    "    b = [] # blue\n",
    "    \n",
    "    for i in samples:\n",
    "        (img, masks) = load_zipped_img(path+'/stage1_train.zip', i) \n",
    "        r.append(np.average(img[:,:,0]))\n",
    "        g.append(np.average(img[:,:,1]))\n",
    "        b.append(np.average(img[:,:,2]))\n",
    "        img = float2int8(grayscale(img))\n",
    "        \n",
    "        # in some cases, image background is bright and cell darker, there needs a inverse of pixel value\n",
    "        if np.average(img) > 125:\n",
    "            img = 255 - img\n",
    "\n",
    "        length = img.shape[0]\n",
    "        len_list.append(length)\n",
    "        width = img.shape[1]\n",
    "        wid_list.append(width)\n",
    "        avg_grey.append(np.average(img))\n",
    "        \n",
    "        # use opencv to find contour and get some statistic data\n",
    "        img = cv2.GaussianBlur(img, (3, 3), 1)\n",
    "        ret, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "        _, cnts, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "        max_cnt_area.append(cv2.contourArea(cnts[0])/length/width)\n",
    "\n",
    "        av = 0\n",
    "        for i in cnts:\n",
    "            av = av + cv2.contourArea(i)\n",
    "        av = av/len(cnts)\n",
    "        \n",
    "        # since different pic has different size, we'd better normalise it \n",
    "        avg_cnt_area.append(av/length/width)\n",
    "        num_cnt.append(len(cnts))\n",
    "        \n",
    "    df = pd.DataFrame({'max_area':max_cnt_area,'average_area':avg_cnt_area,\n",
    "                       'num_cnt':num_cnt,'average_grey':avg_grey,'wid':wid_list,'len':len_list,\n",
    "                       'r':r,'g':g,'b':b\n",
    "                      }) \n",
    "    return df\n",
    "\n",
    "df_kmeans = get_image_info(range(0,n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13d991f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "\n",
    "# train geometry classes\n",
    "input_geometry = np.array(df_kmeans[['max_area','average_area','num_cnt','average_grey','wid','len']])\n",
    "geometry_kmeans = KMeans(n_clusters = 3).fit(input_geometry) \n",
    "df_kmeans['geometry_class'] = geometry_kmeans.labels_\n",
    "\n",
    "# train colour classes\n",
    "input_colour = np.array(df_kmeans[['r','g','b']])\n",
    "colour_kmeans = KMeans(n_clusters = 3).fit(input_colour) \n",
    "df_kmeans['colour_class'] = colour_kmeans.labels_\n",
    "\n",
    "# and then make a combination\n",
    "df_kmeans['overall_class'] = 3*df_kmeans['geometry_class']\n",
    "df_kmeans['overall_class'] = df_kmeans['overall_class'] + df_kmeans['colour_class']\n",
    "\n",
    "# plot and save\n",
    "df_kmeans['overall_class'].hist()\n",
    "plt.title('Training Images Separated by Overall Class')\n",
    "plt.show()\n",
    "\n",
    "df_kmeans.to_csv('summary_kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 55.4 ms, total: 1.78 s\n",
      "Wall time: 1.79 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.zeros(7).reshape(1,7) # Feature vector\n",
    "y_train = np.zeros(1) # True pixel values\n",
    "\n",
    "for i in range(0, n_samples):\n",
    "    (img, masks) = load_zipped_img(path+'/stage1_train.zip', i) # loads image and associated masks\n",
    "\n",
    "    (img_guess, markers, sure_bg, sure_fg, uncertain) = watershed(img)\n",
    "\n",
    "    intensity = grayscale(img)\n",
    "    if i in bad_list: intensity[intensity < 0.5] = [0]\n",
    "    \n",
    "    img_guess = grayscale(img_guess)\n",
    "    \n",
    "    filter_sobel = sobel(grayscale(img))[uncertain==255].reshape(-1,1)\n",
    "    filter_laplace = laplace(grayscale(img))[uncertain==255].reshape(-1,1)\n",
    "    \n",
    "    intensity_raw = intensity[uncertain==255].reshape(-1,1)\n",
    "    watershed_predict = img_guess[uncertain==255].reshape(-1,1)\n",
    "    \n",
    "    img3 = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    r,g,b = cv2.split(img3)\n",
    "    \n",
    "    r = r[uncertain==255].reshape(-1,1)\n",
    "    g = g[uncertain==255].reshape(-1,1)\n",
    "    b = b[uncertain==255].reshape(-1,1)\n",
    "    \n",
    "    feature = np.concatenate((intensity_raw, watershed_predict, filter_sobel, filter_laplace, r, g, b), axis = 1)\n",
    "    y_raw = sum(masks)\n",
    "    y_raw = y_raw[uncertain==255].reshape(-1,1)\n",
    "    \n",
    "    x_train = np.append(x_train, feature).reshape(-1,7)\n",
    "    y_train = np.append(y_train, y_raw).reshape(-1,1)\n",
    "\n",
    "x_train = x_train[1:,:]\n",
    "y_train = y_train[1:,0]\n",
    "\n",
    "clf_improved = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "%time clf_improved.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "Random forest accuracy: improved = 96.83290058185483% ; previous = 96.94593349345611%\n",
      "Random forest precision: improved = 81.42443498778592% ; previous = 82.72161583181739%\n",
      "Random forest f1-score: improved = 84.79211315800211% ; previous = 85.61611241929876%\n",
      "Random forest IoU: improved = 56.09902607585761% ; previous = 57.54850661491695%\n"
     ]
    }
   ],
   "source": [
    "score_rfc_improved = []\n",
    "\n",
    "for i in range(0, n_samples):\n",
    "    (img, masks) = load_zipped_img(path+'/stage1_train.zip', i)\n",
    "\n",
    "    y_real = sum(masks).reshape(-1,1)\n",
    "    shap = grayscale(img).shape\n",
    "    \n",
    "    (img_guess, markers, sure_bg, sure_fg, uncertain) = watershed(img)\n",
    "      \n",
    "    # random forest prediction\n",
    "    intensity = grayscale(img)\n",
    "    if i in bad_list: intensity[intensity < 0.5] = [0]\n",
    "    intensity_raw = intensity.reshape(-1,1)\n",
    "    watershed_raw = grayscale(img_guess).reshape(-1,1)\n",
    "    filter_sobel = sobel(grayscale(img)).reshape(-1,1)\n",
    "    filter_laplace = laplace(grayscale(img)).reshape(-1,1)\n",
    "    img3 = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    ch1,ch2,ch3 = cv2.split(img3)\n",
    "    ch1 = ch1.reshape(-1,1)\n",
    "    ch2 = ch2.reshape(-1,1)\n",
    "    ch3 = ch3.reshape(-1,1)\n",
    "    feature = np.concatenate((intensity_raw, watershed_raw, filter_sobel, filter_laplace, ch1, ch2, ch3), axis = 1)\n",
    "\n",
    "    # random forest score\n",
    "    yp_rfc = clf_improved.predict(feature)\n",
    "    y_real = sum(masks).reshape(-1,1)\n",
    "    acc_rfc = sklearn.metrics.accuracy_score(y_real, yp_rfc)\n",
    "    prec_rfc = sklearn.metrics.precision_score(y_real, yp_rfc)\n",
    "    f1_rfc = sklearn.metrics.f1_score(y_real, yp_rfc)\n",
    "    labels_rfc, nlabels_rfc, label_mask_rfc = separate_obj(grayscale(yp_rfc.reshape(shap[0], shap[1])))\n",
    "    masks_rfc = predicted_mask_list(labels_rfc)\n",
    "    iou_rfc = iou(yp_rfc, masks_rfc, masks)\n",
    "    score_rfc_improved.append([acc_rfc, prec_rfc, f1_rfc, np.mean(iou_rfc)])\n",
    "\n",
    "new = np.array(score_rfc_improved)\n",
    "print(bad_list)\n",
    "print('Random forest accuracy: improved = {0}% ; previous = {1}%'.format(np.mean(new[:,0])*100, np.mean(rfc[:,0])*100))\n",
    "print('Random forest precision: improved = {0}% ; previous = {1}%'.format(np.mean(new[:,1])*100, np.mean(rfc[:,1])*100))\n",
    "print('Random forest f1-score: improved = {0}% ; previous = {1}%'.format(np.mean(new[:,2])*100, np.mean(rfc[:,2])*100))\n",
    "print('Random forest IoU: improved = {0}% ; previous = {1}%'.format(np.mean(new[:,3])*100, np.mean(rfc[:,3])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Example of the Code\n",
    "\n",
    "* Demonstrates the methods involved using a smaller set of 30 training images\n",
    "* Evaluates performance based on a confusion matrix and accuracy (compared to the true masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optional: Data Shape Manipulation\n",
    "\n",
    "* The function one_index takes an image (2d array) and converts it to a 1d array. They are indexed from top to bottom then left to right\n",
    "* The function pad_normalize helps account for variation in image sizes. It determines the maximum length in a set of one-indexed images and \"pads\" all other one-indexed images with zeros so that all images have the same length.\n",
    "\n",
    "# Optional: Scripts to Encode Outputs for Kaggle\n",
    "\n",
    "* The function convert2runlength finds the objects in an image (1 corresponds to object, 0 to background) and finds runs of continuous object pixels\n",
    "* The function rle generates a dataframe of images in run-length format. This is the output format required by the Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-indexes a 2d array into 1d, top down then left right, output is np 1d array\n",
    "def one_index(arr2d):\n",
    "    h, w = arr2d.shape[0:2]\n",
    "    \n",
    "    arr1d = []\n",
    "    for col in range(0, w):\n",
    "        for row in range(0, h):\n",
    "            arr1d.append(arr2d[row][col])\n",
    "    return np.array(arr1d)\n",
    "    \n",
    "# pads all vectors in array to have max_len, returns np array\n",
    "def pad_normalize(array, max_len):\n",
    "    for i in range(0, len(array)):\n",
    "        vec = array[i]\n",
    "        if len(vec) < max_len:\n",
    "            array[i] = np.concatenate(( np.array(vec).reshape(1,-1), np.zeros((1, (max_len-len(vec)))) ), axis=1)\n",
    "        else:\n",
    "            array[i] = np.array(vec).reshape(1,-1)\n",
    "    return np.array(array)\n",
    "\n",
    "# convert path to run-length encoding (RLE) output format\n",
    "def convert2runlength(x):\n",
    "    obj = np.where(x.T.flatten()==1)[0] #1 corresponds to object, 0 to background\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in obj: # find continuous set of object pixels\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return \" \".join([str(i) for i in run_lengths])\n",
    "\n",
    "def rle(img_masked, im_id):\n",
    "    (labels, nlabels, label_mask) = separate_obj(img_masked)\n",
    "    im_df = pd.DataFrame()\n",
    "    for label_num in range(1, nlabels+1):\n",
    "        label_mask = np.where(labels == label_num, 1, 0)\n",
    "        if label_mask.flatten().sum() > 10:\n",
    "            rle = convert2runlength(label_mask)\n",
    "            s = pd.Series({'ImageId': im_id, 'EncodedPixels': rle})\n",
    "            im_df = im_df.append(s, ignore_index=True)\n",
    "    return im_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
