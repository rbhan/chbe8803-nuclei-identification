{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automated Nuclei Identification in Divergent Images\n",
    "\n",
    "### Rebecca Han, Jack Findley\n",
    "\n",
    "### 04.12.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Roadmap\n",
    "***\n",
    "\n",
    "**Motivation**<p>\n",
    "\n",
    "**Project Scope**<br>\n",
    "&emsp; Dataset<br>\n",
    "&emsp; Challenges<br>\n",
    "&emsp; Workflow<p>\n",
    "\n",
    "**Methods**<br>\n",
    "&emsp; Detection<br>\n",
    "&emsp; Segmentation<br>\n",
    "&emsp; Classification<p>\n",
    "\n",
    "**Results**<br>\n",
    "&emsp; Error metrics<p>\n",
    "\n",
    "**Improvement**<br>\n",
    "&emsp; K-means clustering<p>\n",
    "\n",
    "**Conclusion**<br>\n",
    "&emsp; Takeaways<br>\n",
    "&emsp; Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation: Automated Nuclei Identification\n",
    "***\n",
    "\n",
    "600,000 people in the US (0.2% of population) die from cancer every year.\n",
    "\n",
    "40% of all men and women will be diagnosed with cancer at some point in their lives.\n",
    "\n",
    "Early diagnosis is critical to higher treatment success.\n",
    "\n",
    "Cancer cells display different biomarkers, visible in immunohistochemical (IHC) imaging.\n",
    "\n",
    "Rapid and accurate image screening would tremendously improve cancer treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Project Scope\n",
    "***\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "665 training images\n",
    "\n",
    "Each has between 4 to ~380 masks → 32145 files total\n",
    "\n",
    "Largest image size is 1024x1536 → ~1.5 million pixels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 1. Example of a training image with 58 true identified objects (nuclei) as individual and combined masks](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Challenges\n",
    "\n",
    "Variation in imaging condition (illumination, contrast, fluorescence and staining)\n",
    "\n",
    "Variation in cell / tissue type (epithelial, lung, gastrointestinal, etc)\n",
    "\n",
    "Cell aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Workflow\n",
    "\n",
    "![Figure 0. Schematic of project workflow](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methods: Naive to Sophisticated\n",
    "***\n",
    "\n",
    "Detection<br>\n",
    "&emsp; Normalize colour<br>\n",
    "&emsp; Binarize image\n",
    "\n",
    "Segmentation<br>\n",
    "&emsp; Edge finding<br>\n",
    "&emsp; Shape/size fitting - concavity, ellipse, contour\n",
    "\n",
    "Classification, dimensionality reduction<br>\n",
    "&emsp; \"Machine learning\" - cluster analysis, random forests, convolutional neural networks\n",
    "\n",
    "**(1) Random forest classification (2) + K-means clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detection: Otsu's Thresholding\n",
    "***\n",
    "\n",
    "Clustering method\n",
    "\n",
    "Assumes pixel intensity follows bimodal, well-separated distribution\n",
    "\n",
    "Minimize within-class variance  $\\sigma_w^2 (t) = \\omega_0 (t) \\sigma_0^2 (t) + \\omega_1 (t) \\sigma_1^2 (t)$\n",
    "\n",
    "Works well if ~same number of pixels in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 3. Comparison of thresholded objects and total true objects](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It fails dramatically when intensity is not bimodal.\n",
    "\n",
    "![Figure 4. Original training image 177, grayscale, and thresholded](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is most successful when nuclei are solid, distinct and not aggregated.\n",
    "\n",
    "![Figure 5. Original training image 1, grayscale, and thresholded](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Segmentation: Watershed\n",
    "***\n",
    "\n",
    "Find object borders in a grayscale image\n",
    "\n",
    "Topographic map based on brightness\n",
    "\n",
    "Flood the map from sources in each local minimum \n",
    "\n",
    "Build barriers when different water sources meet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Watershed flooding in 1D](https://imagej.net/_images/thumb/c/c5/Watershed-flooding-graph.png/467px-Watershed-flooding-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Watershed is better than thresholding, but notoriously over-segments.\n",
    "\n",
    "![Figure 6. Training image 510 true objects, predicted objects, and markers](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, watershed is a great way to reduce our feature vector.\n",
    "\n",
    "![Figure 7. Training image 510 sure background, sure foreground, and uncertain area](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (1) Random Forest Classification\n",
    "***\n",
    "\n",
    "Decision tree classifiers are supervised learning algorithms\n",
    "\n",
    "Split the data according to some variable, at some value\n",
    "\n",
    "Find key variable and value that minimize variance (entropy) between sets\n",
    "\n",
    "Random forests are ensembles of trees that vote on each pixel's class (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Trained on the uncertain area\n",
    "\n",
    "X_train = feature vector (13,022,247 x 7)\n",
    "* Grayscale pixel intensity (continuous)\n",
    "* Watershed prediction for a pixel (discrete)\n",
    "* Gradient, Laplacian of pixel intensity (discrete)\n",
    "* Intensity of rgb colour channels (continuous)\n",
    "\n",
    "Y_train = sum of true masks (13,022,247 x 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Random forest can correctly predict non-bimodal images with cell overlap.\n",
    "\n",
    "![Figure 8. Comparison of true objects and threshold, watershed, random forest predictions for training image 177](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The random forest classifier performs significantly better. \n",
    "\n",
    "It has \"learned\" to split this image across a higher grayscale intensity. \n",
    "\n",
    "![Figure 9. Comparison of true objects and threshold, watershed, random forest predictions for training image 23](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Random forest also works in cases where thresholding flips objects/background.\n",
    "\n",
    "However, it struggles on noisy or partially occluded images.\n",
    "\n",
    "This is why leading algorithms combine contour fitting with machine learning.\n",
    "\n",
    "![Figure 10. Comparison of true objects and threshold, watershed, random forest predictions for training image 175](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results: Quantifying Error of Different Methods\n",
    "***\n",
    "\n",
    "Multiple ways of reporting error.\n",
    "\n",
    "(1) Confusion matrix\n",
    "\n",
    "(2) Statistical error metrics relevant in binary classification\n",
    "* Accuracy = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / (Total)\n",
    "* Precision = (TP) / (TP + FP)\n",
    "* Recall = (TP) / (TP + FN)\n",
    "* F1 Score = 2$*$(Precision $*$ Recall) / (Precision + Recall)\n",
    "\n",
    "(3) Jaccard index (Intersection over Union) measures similarity between sample sets<br>\n",
    "For a set of predicted object pixels A and true object pixels B,\n",
    "$$IoU(A,B) = \\frac{A \\cap B}{A \\cup B}$$\n",
    "<b>Calculating IoU was a bottleneck (~8.5 hours).</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 19. Confusion matrix of watershed and random forest classifier predictions](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We report error averaged across 665 training images.\n",
    "\n",
    "Watershed may seem better, but this is due to class imbalance.\n",
    "\n",
    "|   | Watershed|Random Forest|\n",
    "|------|------|------|\n",
    "|Accuracy| 95.5% | 96.0% |\n",
    "|Precision| 86.6% | 80.0% |\n",
    "|F1 Score| 82.3% | 80.7% |\n",
    "|IoU| 58.2% | 55.3% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Improvements: Dimensionality Reduction \n",
    "***\n",
    "\n",
    "We flagged images with < 70% accuracy OR precision.\n",
    "\n",
    "The poorly predicted images all tend to be of certain type.\n",
    "\n",
    "Intuitively, dimensionality reduction should work well here.\n",
    "\n",
    "Why did we not start with dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### We did, but dimensionality reduction takes a long time.\n",
    "\n",
    "Training linear SVM took > 10 times longer than training the random forest.\n",
    "\n",
    "Based on complexity analysis, k-means clustering was most efficient.\n",
    "\n",
    "|   | Complexity (of training) | n (# of samples) | d (# of features) |\n",
    "|------|------|------|------|\n",
    "|PCA | $O(n*d^2$+$d^3)$ | 665 images | ~1.5 M pixels in largest image | \n",
    "|SVM (RBF kernel)| $O(n^3)$ [1] | ~13 M reduced pixels | 7 features |\n",
    "|SVM (Linear) | $O(n^2)$ [2] | ~13 M reduced pixels | 7 features |\n",
    "|KMEANS (Lloyd's algorithm)| $O(n*d*$(6 clusters)$*$(300 iterations)$)$ | 665 images | 6 geometry, 3 colour features |\n",
    "|Random Forest |  $O(n*d*$(10 trees)$*$(5 depth)$)$ | ~13 M reduced pixels | 7 features |\n",
    "\n",
    "[1] Srebro and Shalev-Shwartz. (**2008**) *Proceedings of ICML*, doi:10.1.1.139.2112\n",
    "\n",
    "[2] Bordes, et al. (**2005**) *J Mach Learn Res*, 6, 1579–1619, doi:10.1.1.60.9676."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# (2) Random Forest + K-means Clustering\n",
    "***\n",
    "\n",
    "By just looking at the training set, we guess 6 clusters of images.\n",
    "\n",
    "Images in the training set differ based on cell features and colour.\n",
    "\n",
    "Geometry feature vector (6):\n",
    "* average grayscale intensity\n",
    "* image size (width, length)\n",
    "* contour area (max, mean)\n",
    "* number of contours per image\n",
    "\n",
    "Colour feature vector (3):\n",
    "* average red, green, blue intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Watershed does great on Class 0 images, hence why it seems to outperform the random forest.\n",
    "\n",
    "![Figure 20. Histogram of Training Images, Separated by K-means Class](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Retraining on each cluster yields significant improvement\n",
    "\n",
    "Error averaged across all 665 images.\n",
    "\n",
    "The cluster specific random forest performs better than either method.\n",
    "\n",
    "|   | Watershed|Random Forest|**Cluster Specific RF**|\n",
    "|------|------|------|------|\n",
    "|Accuracy| 95.5% | 96.0% | **96.9%** |\n",
    "|Precision| 86.6% | 80.0% | **87.0%** |\n",
    "|F1 Score| 82.3% | 80.7% | **85.9%** |\n",
    "|IoU| 58.2% | 55.3% | **65.9%** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|   | Watershed|Random Forest|**Cluster Specific RF**|\n",
    "|------|------|------|------|\n",
    "|Accuracy| 96.7% | 96.9% | **97.2%** |\n",
    "|Precision| 90.1% | 79.9% | **81.2%** |\n",
    "|F1 Score| 85.0% | 81.1% | **84.3%** |\n",
    "\n",
    "![Figure 13. Example Images Representative of K-means Class 0](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|   | Watershed|Random Forest|**Cluster Specific RF**|\n",
    "|------|------|------|------|\n",
    "|Accuracy| 94.7% | 95.0% | **95.4%** |\n",
    "|Precision| 84.9% | 87.4% | **86.1%** |\n",
    "|F1 Score| 84.3% | 83.2% | **85.8%** |\n",
    "\n",
    "![Figure 14. Example Images Representative of K-means Class 1](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|   | Watershed|Random Forest|**Cluster Specific RF**|\n",
    "|------|------|------|------|\n",
    "|Accuracy| 91.1% | 92.3% | **93.2%** |\n",
    "|Precision| 79.6% | 78.2% | **81.9%** |\n",
    "|F1 Score| 78.3% | 80.1% | **80.7%** |\n",
    "\n",
    "![Figure 15. Example Images Representative of K-means Class 2](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|   | Watershed|Random Forest|**Cluster Specific RF**|\n",
    "|------|------|------|------|\n",
    "|Accuracy| 84.2% | 83.1% | **97.4%** |\n",
    "|Precision| 36.5% | 38.8% | **80.5%** |\n",
    "|F1 Score| 48.5% | 42.8% | **76.2%** |\n",
    "\n",
    "![Figure 16. Example Images Representative of K-means Class 3](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|   | Watershed|Random Forest|**Cluster Specific RF**|\n",
    "|------|------|------|------|\n",
    "|Accuracy| 80.6% | 97.7% | **98.9%** |\n",
    "|Precision| 8.7% | 48.5% | **72.2%** |\n",
    "|F1 Score| 16.0% | 59.8% | **69.0%** |\n",
    "\n",
    "![Figure 17. Example Images Representative of K-means Class 4](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|   | Watershed|Random Forest|**Cluster Specific RF**|\n",
    "|------|------|------|------|\n",
    "|Accuracy| 96.2% | 96.1% | **96.7%** |\n",
    "|Precision| 92.9% | 88.8% | **89.4%** |\n",
    "|F1 Score| 86.3% | 86.2% | **88.6%** |\n",
    "\n",
    "![Figure 18. Example Images Representative of K-means Class 5](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 11. Comparison of true objects, watershed, random forest, and cluster-specific random forest predictions for training image 23](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure 12. Comparison of true objects, watershed, random forest, and cluster-specific random forest predictions for training image 175](https://raw.githubusercontent.com/rbhan/chbe8803-nuclei-identification/master/images/fig12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "***\n",
    "\n",
    "#### Takeaways\n",
    "\n",
    "Models typically over-predict nucleus size (many false positives)\n",
    "\n",
    "No \"one-size-fits-all\" model for all cell type/imaging conditions\n",
    "\n",
    "Image analysis gets very costly very fast, so pay attention to algorithm complexity\n",
    "\n",
    "This is a hard problem, with a lot of money in it ($100k prize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Future Work\n",
    "\n",
    "Improving class balance - train on more images (IHC data is readily available)\n",
    "\n",
    "Improving algorithms - add iterative PCA, neural networks, shape/size fitting\n",
    "\n",
    "Improving runtime - parallelize, run on a cluster (PACE has weird packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We are happy to take questions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
